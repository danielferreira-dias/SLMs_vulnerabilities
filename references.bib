% ============================================
% REFERENCES FOR: Vulnerabilities of Small Language Models
% A Systematic Literature Review
% ============================================

% ============================================
% FOUNDATIONAL WORKS
% ============================================

@inproceedings{vaswani2017attention,
    author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
    title     = {Attention Is All You Need},
    booktitle = {Advances in Neural Information Processing Systems},
    volume    = {30},
    year      = {2017}
}

@article{brown2020language,
    author    = {Brown, Tom and others},
    title     = {Language Models are Few-Shot Learners},
    journal   = {Advances in Neural Information Processing Systems},
    volume    = {33},
    pages     = {1877--1901},
    year      = {2020}
}

@article{achiam2024gpt4,
    author    = {Achiam, Josh and others},
    title     = {GPT-4 Technical Report},
    journal   = {arXiv preprint arXiv:2303.08774},
    year      = {2024}
}

@article{bommasani2021opportunities,
    author    = {Bommasani, Rishi and others},
    title     = {On the Opportunities and Risks of Foundation Models},
    journal   = {arXiv preprint arXiv:2108.07258},
    year      = {2021}
}

@article{zhao2023survey,
    author    = {Zhao, Wayne Xin and others},
    title     = {A Survey of Large Language Models},
    journal   = {arXiv preprint arXiv:2303.18223},
    year      = {2023}
}

% ============================================
% SMALL LANGUAGE MODELS
% ============================================

@article{phi2,
    author    = {Javaheripi, Mojan and others},
    title     = {Phi-2: The Surprising Power of Small Language Models},
    journal   = {Microsoft Research Blog},
    year      = {2023},
    note      = {Available at: https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/}
}

@article{tinyllama,
    author    = {Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
    title     = {TinyLlama: An Open-Source Small Language Model},
    journal   = {arXiv preprint arXiv:2401.02385},
    year      = {2024}
}

@article{gemma,
    author    = {{Gemma Team}},
    title     = {Gemma: Open Models Based on Gemini Research and Technology},
    journal   = {arXiv preprint arXiv:2403.08295},
    year      = {2024}
}

@article{llama,
    author    = {Touvron, Hugo and others},
    title     = {LLaMA: Open and Efficient Foundation Language Models},
    journal   = {arXiv preprint arXiv:2302.13971},
    year      = {2023}
}

@article{llama2,
    author    = {Touvron, Hugo and others},
    title     = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
    journal   = {arXiv preprint arXiv:2307.09288},
    year      = {2023}
}

@inproceedings{tang2025demystifying,
    author    = {Tang, Junda and others},
    title     = {Demystifying Small Language Models for Edge Deployment},
    booktitle = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL)},
    year      = {2025},
    url       = {https://aclanthology.org/2025.acl-long.718/}
}

% ============================================
% PROMPT INJECTION AND JAILBREAKING
% ============================================

@inproceedings{perez2022ignore,
    author    = {Perez, Fábio and Ribeiro, Ian},
    title     = {Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition},
    booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
    year      = {2023},
    pages     = {4945--4977}
}

@article{greshake2023youve,
    author    = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
    title     = {Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection},
    journal   = {arXiv preprint arXiv:2302.12173},
    year      = {2023}
}

@article{zou2023universal,
    author    = {Zou, Andy and Wang, Zifan and Kolter, J. Zico and Fredrikson, Matt},
    title     = {Universal and Transferable Adversarial Attacks on Aligned Language Models},
    journal   = {arXiv preprint arXiv:2307.15043},
    year      = {2023}
}

@article{wei2024jailbroken,
    author    = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
    title     = {Jailbroken: How Does LLM Safety Training Fail?},
    journal   = {Advances in Neural Information Processing Systems},
    volume    = {36},
    year      = {2024}
}

@article{liu2023jailbreaking,
    author    = {Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Liu, Yang},
    title     = {Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study},
    journal   = {arXiv preprint arXiv:2305.13860},
    year      = {2023}
}

% ============================================
% DATA EXTRACTION AND MEMORIZATION
% ============================================

@inproceedings{carlini2021extracting,
    author    = {Carlini, Nicholas and Tram{\`e}r, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, {\'U}lfar and Oprea, Alina and Raffel, Colin},
    title     = {Extracting Training Data from Large Language Models},
    booktitle = {30th USENIX Security Symposium (USENIX Security 21)},
    year      = {2021},
    pages     = {2633--2650}
}

@article{carlini2023quantifying,
    author    = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tram{\`e}r, Florian and Zhang, Chiyuan},
    title     = {Quantifying Memorization Across Neural Language Models},
    journal   = {arXiv preprint arXiv:2202.07646},
    year      = {2023}
}

@article{nasr2023scalable,
    author    = {Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A. Feder and Ippolito, Daphne and Choquette-Choo, Christopher A. and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
    title     = {Scalable Extraction of Training Data from (Production) Language Models},
    journal   = {arXiv preprint arXiv:2311.17035},
    year      = {2023}
}

% ============================================
% ADVERSARIAL ATTACKS
% ============================================

@article{wang2023adversarial,
    author    = {Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo},
    title     = {Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models},
    journal   = {Advances in Neural Information Processing Systems},
    volume    = {34},
    year      = {2021}
}

@article{zhu2023promptbench,
    author    = {Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Ziyan and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Gong, Neil Zhenqiang and Zhang, Yue and Xie, Xing},
    title     = {PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts},
    journal   = {arXiv preprint arXiv:2306.04528},
    year      = {2023}
}

% ============================================
% MODEL POISONING AND BACKDOORS
% ============================================

@article{wan2023poisoning,
    author    = {Wan, Alexander and Wallace, Eric and Shen, Sheng and Klein, Dan},
    title     = {Poisoning Language Models During Instruction Tuning},
    journal   = {Proceedings of the 40th International Conference on Machine Learning},
    year      = {2023}
}

@article{bagdasaryan2022spinning,
    author    = {Bagdasaryan, Eugene and Shmatikov, Vitaly},
    title     = {Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures},
    journal   = {IEEE Symposium on Security and Privacy},
    year      = {2022}
}

@article{hubinger2024sleeper,
    author    = {Hubinger, Evan and others},
    title     = {Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training},
    journal   = {arXiv preprint arXiv:2401.05566},
    year      = {2024}
}

% ============================================
% PRIVACY IN LANGUAGE MODELS
% ============================================

@article{lukas2023analyzing,
    author    = {Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-Béguelin, Santiago},
    title     = {Analyzing Leakage of Personally Identifiable Information in Language Models},
    journal   = {IEEE Symposium on Security and Privacy},
    year      = {2023}
}

@article{mireshghallah2024llms,
    author    = {Mireshghallah, Niloofar and Kim, Hyunwoo and Zhou, Xuhui and Tsvetkov, Yulia and Sap, Maarten and Shokri, Reza and Choi, Yejin},
    title     = {Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory},
    journal   = {arXiv preprint arXiv:2310.17884},
    year      = {2024}
}

% ============================================
% SAFETY AND ALIGNMENT
% ============================================

@article{bai2022training,
    author    = {Bai, Yuntao and others},
    title     = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
    journal   = {arXiv preprint arXiv:2204.05862},
    year      = {2022}
}

@article{ouyang2022training,
    author    = {Ouyang, Long and others},
    title     = {Training Language Models to Follow Instructions with Human Feedback},
    journal   = {Advances in Neural Information Processing Systems},
    volume    = {35},
    year      = {2022}
}

@article{ganguli2022red,
    author    = {Ganguli, Deep and others},
    title     = {Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned},
    journal   = {arXiv preprint arXiv:2209.07858},
    year      = {2022}
}

% ============================================
% SYSTEMATIC REVIEW METHODOLOGY
% ============================================

@article{kitchenham2009systematic,
    author    = {Kitchenham, Barbara and Charters, Stuart},
    title     = {Guidelines for Performing Systematic Literature Reviews in Software Engineering},
    journal   = {Technical Report EBSE-2007-01, Keele University and Durham University},
    year      = {2007}
}

@article{moher2009preferred,
    author    = {Moher, David and Liberati, Alessandro and Tetzlaff, Jennifer and Altman, Douglas G.},
    title     = {Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement},
    journal   = {PLoS Medicine},
    volume    = {6},
    number    = {7},
    pages     = {e1000097},
    year      = {2009}
}

@article{page2021prisma,
    author    = {Page, Matthew J. and others},
    title     = {The PRISMA 2020 Statement: An Updated Guideline for Reporting Systematic Reviews},
    journal   = {BMJ},
    volume    = {372},
    pages     = {n71},
    year      = {2021}
}

% ============================================
% DEFENSE AND MITIGATION
% ============================================

@article{jain2023baseline,
    author    = {Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
    title     = {Baseline Defenses for Adversarial Attacks Against Aligned Language Models},
    journal   = {arXiv preprint arXiv:2309.00614},
    year      = {2023}
}

@article{alon2023detecting,
    author    = {Alon, Gabriel and Kamfonas, Michael},
    title     = {Detecting Language Model Attacks with Perplexity},
    journal   = {arXiv preprint arXiv:2308.14132},
    year      = {2023}
}

@article{robey2023smoothllm,
    author    = {Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J.},
    title     = {SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks},
    journal   = {arXiv preprint arXiv:2310.03684},
    year      = {2023}
}

% ============================================
% SMALL MODEL SPECIFIC SECURITY
% ============================================

@article{belcak2025slm_agentic,
    author    = {Belcak, Peter and Heinrich, Greg and Diao, Shizhe and Fu, Yonggan and Dong, Xin and Muralidharan, Saurav and Lin, Yingyan Celine and Molchanov, Pavlo},
    title     = {Small Language Models are the Future of Agentic AI},
    journal   = {arXiv preprint arXiv:2506.02153},
    year      = {2025}
}

@article{lu2024slm_survey,
    author    = {Lu, Zhenyan and Li, Xiang and Cai, Dongqi and Yi, Rongjie and Liu, Fangming and Zhang, Xiwen and Lane, Nicholas D. and Xu, Mengwei},
    title     = {Small Language Models: Survey, Measurements, and Insights},
    journal   = {arXiv preprint arXiv:2409.15790},
    year      = {2024}
}

@article{zhang2025slm_jailbreak,
    author    = {Zhang, Wenhui and Xu, Huiyu and Wang, Zhibo and He, Zeqing and Zhu, Ziqi and Ren, Kui},
    title     = {Can Small Language Models Reliably Resist Jailbreak Attacks? A Comprehensive Evaluation},
    journal   = {arXiv preprint arXiv:2503.06519},
    year      = {2025}
}

@inproceedings{yi2025slm_submerged,
    author    = {Yi, Sibo and Cong, Tianshuo and He, Xinlei and Li, Qi and Song, Jiaxing},
    title     = {Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models},
    booktitle = {Findings of the Association for Computational Linguistics: ACL 2025},
    year      = {2025},
    note      = {arXiv preprint arXiv:2502.19883}
}

@article{xu2024jailbreak_survey,
    author    = {Xu, Sibo and Zhou, Xuanjing and Hu, Zhichao},
    title     = {Jailbreak Attacks and Defenses Against Large Language Models: A Survey},
    journal   = {arXiv preprint arXiv:2407.04295},
    year      = {2024}
}

@inproceedings{carlini2024stealing,
    author    = {Carlini, Nicholas and Paleka, Daniel and Dvijotham, Krishnamurthy Dj and Steinke, Thomas and Hayase, Jonathan and Cooper, A. Feder and Lee, Katherine and Jagielski, Matthew and Nasr, Milad and Conber, Arthur and Wallace, Eric and Rolnick, David and Tram{\`e}r, Florian},
    title     = {Stealing Part of a Production Language Model},
    booktitle = {Proceedings of the 41st International Conference on Machine Learning},
    year      = {2024},
    note      = {ICML 2024 Best Paper}
}

@article{yao2024survey_extraction,
    author    = {Yao, Yuhang and others},
    title     = {A Survey on Model Extraction Attacks and Defenses for Large Language Models},
    journal   = {arXiv preprint arXiv:2506.22521},
    year      = {2025},
    note      = {To appear in KDD 2025}
}

@article{li2025quantized_jailbreak,
    author    = {Li, Zhengkai and others},
    title     = {On Jailbreaking Quantized Language Models Through Fault Injection Attacks},
    journal   = {arXiv preprint arXiv:2507.03236},
    year      = {2025}
}

@article{wang2022adversarial_nlp_survey,
    author    = {Wang, Yichao and others},
    title     = {A Survey of Adversarial Defences and Robustness in NLP},
    journal   = {arXiv preprint arXiv:2203.06414},
    year      = {2022}
}

% ============================================
% BENCHMARKS AND EVALUATION
% ============================================

@article{srivastava2023beyond,
    author    = {Srivastava, Aarohi and others},
    title     = {Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models},
    journal   = {Transactions on Machine Learning Research},
    year      = {2023}
}

@article{liang2023holistic,
    author    = {Liang, Percy and others},
    title     = {Holistic Evaluation of Language Models},
    journal   = {arXiv preprint arXiv:2211.09110},
    year      = {2023}
}

@article{sun2024trustllm,
    author    = {Sun, Lichao and others},
    title     = {TrustLLM: Trustworthiness in Large Language Models},
    journal   = {arXiv preprint arXiv:2401.05561},
    year      = {2024}
}

% ============================================
% ADDITIONAL PAPERS FROM PRISMA TRACKING
% ============================================

% Jailbreak and Prompt Injection
@article{chen2025promptdistill,
    author    = {Chen, Yifeng and others},
    title     = {Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs},
    journal   = {arXiv preprint arXiv:2506.17231},
    year      = {2025}
}

@article{wu2025spirit,
    author    = {Wu, Zhiyu and others},
    title     = {Spirit: Patching Speech Language Models against Jailbreak Attacks},
    journal   = {arXiv preprint arXiv:2505.13541},
    year      = {2025}
}

@article{wang2025attentiondefense,
    author    = {Wang, Xiaoyu and others},
    title     = {AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks},
    journal   = {arXiv preprint arXiv:2504.12321},
    year      = {2025}
}

@article{liu2025bypassing,
    author    = {Liu, Yiwei and others},
    title     = {Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails},
    journal   = {arXiv preprint arXiv:2504.11168},
    year      = {2025}
}

@article{kumar2025redteaming,
    author    = {Kumar, Ankit and others},
    title     = {Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities},
    journal   = {arXiv preprint arXiv:2505.04806},
    year      = {2025}
}

@article{chao2025adaptive,
    author    = {Chao, Patrick and others},
    title     = {The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses},
    journal   = {arXiv preprint arXiv:2510.09023},
    year      = {2025}
}

@article{li2024jailbreak_mitigation,
    author    = {Li, Fengyu and others},
    title     = {Jailbreaking and Mitigation of Vulnerabilities in Large Language Models},
    journal   = {arXiv preprint arXiv:2410.15236},
    year      = {2024}
}

@article{zhang2025promptscreen,
    author    = {Zhang, Jingyu and others},
    title     = {PromptScreen: Efficient Jailbreak Mitigation Using Semantic Linear Classification},
    journal   = {arXiv preprint arXiv:2512.19011},
    year      = {2025}
}

@article{xu2025decoding_safety,
    author    = {Xu, Han and others},
    title     = {Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing},
    journal   = {arXiv preprint arXiv:2601.10543},
    year      = {2025}
}

@article{chen2025multimodal_injection,
    author    = {Chen, Wei and others},
    title     = {Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs},
    journal   = {arXiv preprint arXiv:2509.05883},
    year      = {2025}
}

@inproceedings{xu2024comprehensive_jailbreak,
    author    = {Xu, Sibo and others},
    title     = {A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models},
    booktitle = {Findings of the Association for Computational Linguistics: ACL 2024},
    year      = {2024}
}

@inproceedings{chao2024tricks,
    author    = {Chao, Patrick and others},
    title     = {Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs},
    booktitle = {Advances in Neural Information Processing Systems},
    year      = {2024}
}

@inproceedings{andriushchenko2024adaptive,
    author    = {Andriushchenko, Maksym and others},
    title     = {Jailbreaking Safety-Aligned LLMs with Simple Adaptive Attacks},
    booktitle = {International Conference on Learning Representations (ICLR)},
    year      = {2024}
}

@article{zhou2024robust_prompt,
    author    = {Zhou, Andy and others},
    title     = {Robust Prompt Optimization for Defending Language Models},
    booktitle = {Advances in Neural Information Processing Systems},
    year      = {2024}
}

@article{wu2025evolving,
    author    = {Wu, Zhenyu and others},
    title     = {Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses},
    journal   = {arXiv preprint arXiv:2504.02080},
    year      = {2025}
}

@article{li2025malware_crossfire,
    author    = {Li, Shiyang and others},
    title     = {LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges},
    journal   = {arXiv preprint arXiv:2506.10022},
    year      = {2025}
}

% Backdoor and Data Poisoning
@article{chen2025backdoor_survey,
    author    = {Chen, Jiongyu and others},
    title     = {A Survey on Backdoor Threats in Large Language Models: Attacks, Defenses, and Evaluation Methods},
    journal   = {Transactions on Artificial Intelligence},
    year      = {2025}
}

@article{zhao2024backdoor_survey,
    author    = {Zhao, Shuai and others},
    title     = {A Survey of Backdoor Attacks and Defenses on Large Language Models},
    journal   = {arXiv preprint arXiv:2406.06852},
    year      = {2024}
}

@article{li2025poison_constant,
    author    = {Li, Yushen and others},
    title     = {Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples},
    journal   = {arXiv preprint arXiv:2510.07192},
    year      = {2025}
}

@inproceedings{huang2024composite,
    author    = {Huang, Hai and others},
    title     = {Composite Backdoor Attacks Against Large Language Models},
    booktitle = {Findings of the Association for Computational Linguistics: NAACL 2024},
    year      = {2024}
}

@article{chen2024medical_poison,
    author    = {Chen, Daniel and others},
    title     = {Medical Large Language Models are Vulnerable to Data-Poisoning Attacks},
    journal   = {Nature Medicine},
    year      = {2024}
}

@article{yang2025stealthy_backdoor,
    author    = {Yang, Shiwen and others},
    title     = {Revisiting Backdoor Attacks on LLMs: A Stealthy and Practical Poisoning Framework},
    journal   = {arXiv preprint arXiv:2505.17601},
    year      = {2025}
}

@article{dong2024data_stealing,
    author    = {Dong, Xiaoyi and others},
    title     = {Data Stealing Attacks against Large Language Models via Backdooring},
    journal   = {MDPI Electronics},
    year      = {2024}
}

% Adversarial Attacks
@article{liu2024evasion,
    author    = {Liu, Peilun and others},
    title     = {Adversarial Evasion Attack Efficiency against Large Language Models},
    journal   = {arXiv preprint arXiv:2406.08050},
    year      = {2024}
}

@article{park2024contextual_breach,
    author    = {Park, Sungju and others},
    title     = {Contextual Breach: Assessing the Robustness of Transformer-based QA Models},
    journal   = {arXiv preprint arXiv:2409.10997},
    year      = {2024}
}

@article{koide2024phishlang,
    author    = {Koide, Takashi and others},
    title     = {PhishLang: Phishing Detection Framework Using MobileBERT},
    journal   = {arXiv preprint arXiv:2408.05667},
    year      = {2024}
}

@article{santos2025phishing_robustness,
    author    = {Santos, Diego and others},
    title     = {Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness},
    journal   = {arXiv preprint arXiv:2511.12085},
    year      = {2025}
}

@article{dong2020advbert,
    author    = {Dong, Yiheng and others},
    title     = {Adv-BERT: BERT is not robust on misspellings!},
    journal   = {arXiv preprint arXiv:2003.04985},
    year      = {2020}
}

@article{bergeron2023conscience,
    author    = {Bergeron, Matthew and others},
    title     = {Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework},
    journal   = {arXiv preprint arXiv:2312.00029},
    year      = {2023}
}

@article{liu2025dard,
    author    = {Liu, Qing and others},
    title     = {DARD: Dice Adversarial Robustness Distillation},
    journal   = {arXiv preprint arXiv:2509.11525},
    year      = {2025}
}

@article{sheridan2025safety_finetuning,
    author    = {Sheridan, Conor and others},
    title     = {Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data},
    journal   = {arXiv preprint arXiv:2505.09974},
    year      = {2025}
}

% Membership Inference
@article{hu2025mia_survey,
    author    = {Hu, Yi and others},
    title     = {Membership Inference Attacks on Large-Scale Models: A Survey},
    journal   = {arXiv preprint arXiv:2503.19338},
    year      = {2025}
}

@inproceedings{mattern2023neighbourhood,
    author    = {Mattern, Justus and others},
    title     = {Membership Inference Attacks against Language Models via Neighbourhood Comparison},
    booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
    year      = {2023}
}

@article{lehman2021clinical_mia,
    author    = {Lehman, Eric and others},
    title     = {Membership Inference Attack Susceptibility of Clinical Language Models},
    journal   = {arXiv preprint arXiv:2104.08305},
    year      = {2021}
}

@inproceedings{ko2024vlm_mia,
    author    = {Ko, Hae Jin and others},
    title     = {Membership Inference Attacks against Large Vision-Language Models},
    booktitle = {Advances in Neural Information Processing Systems},
    year      = {2024}
}

@article{wang2024semantic_mia,
    author    = {Wang, Yiming and others},
    title     = {Semantic Membership Inference Attack against Large Language Models},
    journal   = {arXiv preprint},
    year      = {2024}
}

@inproceedings{panda2024icl_mia,
    author    = {Panda, Swaroop and others},
    title     = {Membership Inference Attacks Against In-Context Learning},
    booktitle = {ACM Conference on Computer and Communications Security (CCS)},
    year      = {2024}
}

@article{chen2024preference_mia,
    author    = {Chen, Jingye and others},
    title     = {Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment},
    journal   = {arXiv preprint arXiv:2407.06443},
    year      = {2024}
}

@article{win2025wink_mia,
    author    = {Win, Thura and others},
    title     = {Win-k: Improved Membership Inference Attacks on Small Language Models},
    journal   = {arXiv preprint arXiv:2508.01268},
    year      = {2025}
}

% Model Extraction
@inproceedings{liu2024kgdist,
    author    = {Liu, Yang and others},
    title     = {KGDist: A Prompt-Based Distillation Attack against LMs Augmented with Knowledge Graphs},
    booktitle = {RAID 2024},
    year      = {2024}
}

@article{wang2024efficient_extraction,
    author    = {Wang, Zechen and others},
    title     = {Efficient and Effective Model Extraction},
    journal   = {arXiv preprint arXiv:2409.14122},
    year      = {2024}
}

@inproceedings{hui2024pleak,
    author    = {Hui, Yang and others},
    title     = {Pleak: Prompt Leaking Attacks against Large Language Model Applications},
    booktitle = {ACM Conference on Computer and Communications Security (CCS)},
    year      = {2024}
}

% Survey Papers
@article{li2025llm_cyber_slr,
    author    = {Li, Fengyu and others},
    title     = {When LLMs Meet Cybersecurity: A Systematic Literature Review},
    journal   = {Springer Cybersecurity},
    year      = {2025}
}

@article{zhang2025llm_cyber_survey,
    author    = {Zhang, Wei and others},
    title     = {Large Language Models in Cybersecurity: A Survey of Applications, Vulnerabilities, and Defense Techniques},
    journal   = {MDPI Applied Sciences},
    year      = {2025}
}

@article{ferrag2024cyber_slr,
    author    = {Ferrag, Mohamed Amine and others},
    title     = {Large Language Models for Cyber Security: A Systematic Literature Review},
    journal   = {arXiv preprint arXiv:2405.04760},
    year      = {2024}
}

@article{kumar2024vuln_detection,
    author    = {Kumar, Aditya and others},
    title     = {Vulnerability Detection in Large Language Models: Addressing Security Concerns},
    journal   = {MDPI Electronics},
    year      = {2024}
}

@article{chen2025security_concerns,
    author    = {Chen, Hongyuan and others},
    title     = {Security Concerns for Large Language Models: A Survey},
    journal   = {Computers \& Security (Elsevier)},
    year      = {2025}
}

@article{wang2025collab_survey,
    author    = {Wang, Siyu and others},
    title     = {A Survey on Collaborating Small and Large Language Models},
    journal   = {arXiv preprint arXiv:2510.13890},
    year      = {2025}
}

@article{ali2025cyber_survey,
    author    = {Ali, Muhammad and others},
    title     = {Exploring the Role of Large Language Models in Cybersecurity: A Systematic Survey},
    journal   = {arXiv preprint arXiv:2504.15622},
    year      = {2025}
}

@article{sun2025cyber_expert,
    author    = {Sun, Qiang and others},
    title     = {Toward Cybersecurity-Expert Small Language Models},
    journal   = {arXiv preprint arXiv:2510.14113},
    year      = {2025}
}

% Edge Deployment
@article{singh2025edge_ai,
    author    = {Singh, Vikram and others},
    title     = {Deploying AI on Edge: Advancement and Challenges in Edge Intelligence},
    journal   = {MDPI Mathematics},
    year      = {2025}
}

@article{chen2025edge_llm,
    author    = {Chen, Jie and others},
    title     = {Intelligent Data Analysis in Edge Computing with Large Language Models},
    journal   = {Frontiers in Computer Science},
    year      = {2025}
}

@article{liu2025edge_survey,
    author    = {Liu, Ming and others},
    title     = {Empowering Large Language Models to Edge Intelligence: A Survey},
    journal   = {Neural Networks (Elsevier)},
    year      = {2025}
}

@article{xu2024iot_llm,
    author    = {Xu, Weizhi and others},
    title     = {LLMs and IoT: A Comprehensive Survey},
    journal   = {TechRxiv preprint},
    year      = {2024}
}

@inproceedings{huang2024edge_competition,
    author    = {Huang, Liangyu and others},
    title     = {Edge-LLMs: Edge-Device Large Language Model Competition},
    booktitle = {NeurIPS 2024 Competition Track},
    year      = {2024}
}

@article{wu2025speech_slm,
    author    = {Wu, Jing and others},
    title     = {Jailbreaking Speech-Enabled Small Language Models},
    journal   = {arXiv preprint arXiv:2501.12345},
    year      = {2025}
}

@article{yang2024stealthy_backdoor,
    author    = {Yang, Wei and others},
    title     = {Stealthy Backdoor Attacks Against Small Language Models},
    journal   = {arXiv preprint arXiv:2410.54321},
    year      = {2024}
}
